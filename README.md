# Investigating the Negative Effects of ChatGPT

After ChatGPT 3.5 and 4.0 launched, Artificial Intelligence Generated Content (AIGC) has been prevalent in several fields. The proficiency of these large language models (LLMs) is impressive in handling natural language processing and coding tasks. However, there are still some risks. Given ChatGPT’s acumen in both coding and language processing, it is possible that even the the current state-of-the-art language models can be deceived, leading to the failure of malicious content detection. Consequently, quantifying the potential adverse effects and preparing a mitigation plan is necessary. Notably, as the latest version of ChatGPT was released less than one year ago, the studies with comprehensive investigations into its potential impacts are still at initial stage.

To identify how the latest models are affected by ChatGPT, the project implements several experiments. Employing three cutting-edge language models, specifically tailored for fake review detection, aggressive language detection and phishing email detection, the study evaluates performance metrics using test datasets generated by ChatGPT.  These results are compared with those obtained from common datasets. Moreover, a solution, leveraging datasets derived from ChatGPT for training data, is tested.  The results offer insights into the efficiency of this strategy in preventing negative impacts from ChatGPT.

The research hypothesis is that the modern language model’s performance on malicious detection is affected negatively by ChatGPT. However, the magnitude of this influence might be different across the three areas. The solution deploying training datasets generated from ChatGPT  is practical to increase models' resilience against such adverse effects.



